{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os, re\n",
    "import psutil\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# keras module for building LSTM \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.utils as ku \n",
    "\n",
    "# for pre-trained embeddings\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file = open(os.path.join(\"drive/My Drive\", \"data/reddit_train_test_capped.pkl\"),'rb')\n",
    "train_df, test_df= pickle.load(final_file),  pickle.load(final_file)\n",
    "\n",
    "final_file.close()\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to only the fields I will need\n",
    "train_df = train_df[['score','body','is_popular']]\n",
    "test_df = test_df[['score','body','is_popular']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nan in body, the input\n",
    "train_df.dropna(subset=['body'], inplace=True)\n",
    "test_df.dropna(subset=['body'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google News embeddings based on 3M words in 300 dimensions\n",
    "filename = os.path.join(\"drive/My Drive\", \"data/GoogleNews-vectors-negative300.bin\")\n",
    "gensim_embeddings = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "\n",
    "pretrained_weights = gensim_embeddings.wv.syn0\n",
    "vocab_size, embedding_size = pretrained_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(os.path.join(\"drive/My Drive\", \"data/all_tweets_clean_v2.csv\"))\n",
    "\n",
    "_, t_test_df = train_test_split(df_data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "  txt = re.sub(r'https:\\/\\/t[.]co\\/[A-Za-z0-9]*$', '', txt)\n",
    "  txt = re.sub(r'\\n', ' ', txt)\n",
    "  txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "  txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "  txt = re.sub(' +', ' ', txt)\n",
    "  return(txt)\n",
    "\n",
    "train_corpus = train_df['body'].apply(clean_text)\n",
    "test_corpus = test_df['body'].apply(clean_text)\n",
    "t_test_corpus = t_test_df['body'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "# tokenize our text\n",
    "tokenizer.fit_on_texts(train_corpus)\n",
    "# turn text into token sequence\n",
    "train_sequences = tokenizer.texts_to_sequences(train_corpus)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_corpus)\n",
    "t_test_sequences = tokenizer.texts_to_sequences(t_test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Sequences and Obtaining Variables: Predictors and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(train_sequences, maxlen = 100)\n",
    "x_test = pad_sequences(test_sequences, maxlen = 100)\n",
    "t_x_test = pad_sequences(t_test_sequences, maxlen = 100)\n",
    "print(t_x_test.shape)\n",
    "\n",
    "y_train = train_df['is_popular'].tolist()\n",
    "y_test = test_df['is_popular'].tolist()\n",
    "t_y_test = t_test_df['is_popular'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model2(embedding_vectors):\n",
    "  model = Sequential()\n",
    "  \n",
    "  \n",
    "  model.add(Embedding(input_dim=vocab_size,\n",
    "                      output_dim=embedding_size,\n",
    "                      weights=[pretrained_weights],\n",
    "                      trainable=False,\n",
    "                      name='embedding_layer'))\n",
    "  \n",
    "  model.add(LSTM(100))\n",
    "  \n",
    "  #model.add(Dropout(0.1))\n",
    "  \n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  \n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "  return(model)\n",
    "\n",
    "model = create_model2(embedding_vectors=100)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy:{}\".format(scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_test_df=os.path.join(\"drive/My Drive\", \"data/reddit_train_test_capped.pkl\")\n",
    "twitter_test_df=os.path.join(\"drive/My Drive\", \"data/twitter_train_test_smaller_v2.pkl\")\n",
    "  \n",
    "final_file = open(reddit_test_df,'rb')\n",
    "_ , reddit_test_df =  pickle.load(final_file),  pickle.load(final_file)\n",
    "final_file.close()\n",
    "  \n",
    "final_file = open(twitter_test_df,'rb')\n",
    "_ , twitter_test_df =  pickle.load(final_file),  pickle.load(final_file)\n",
    "final_file.close()\n",
    "  \n",
    "mod = \"lstm_sentence_classification\"\n",
    "\n",
    "reddit_test_df.dropna(subset=['body'], inplace=True)\n",
    "\n",
    "reddit_test_df[mod] = model.predict(x_test)\n",
    "print(\"twitter_test_df\", twitter_test_df.shape)\n",
    "print(\"t_x_test\", t_x_test.shape)\n",
    "twitter_test_df[mod] = model.predict(t_x_test)\n",
    "    \n",
    "reddit_test_df.to_csv('drive/My Drive/models/reddit_test_predictions_mj.csv',index=False)\n",
    "twitter_test_df.to_csv('drive/My Drive/models/twitter_test_predictions_mj.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
